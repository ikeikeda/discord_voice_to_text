import logging
import re
import os
from typing import List, Dict, Tuple, Optional
from collections import Counter, defaultdict
from dataclasses import dataclass
from .llm_providers import LLMProvider

logger = logging.getLogger(__name__)


@dataclass
class KeywordInfo:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±"""
    word: str
    frequency: int
    importance_score: float
    category: str
    contexts: List[str]  # å‡ºç¾æ–‡è„ˆ


@dataclass
class ActionItem:
    """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ """
    task: str
    assignee: Optional[str]
    deadline: Optional[str]
    priority: str
    context: str


class KeywordExtractor:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ»åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        self.llm_provider = llm_provider
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®æœ‰åŠ¹/ç„¡åŠ¹è¨­å®š
        self.enable_keyword_extraction = os.getenv("ENABLE_KEYWORD_EXTRACTION", "true").lower() == "true"
        self.enable_action_items = os.getenv("ENABLE_ACTION_ITEMS", "true").lower() == "true"
        
        # é™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªå˜èªï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼‰
        self.stop_words = {
            'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹',
            'ã¨ã„ã†', 'ã«ã¤ã„ã¦', 'ã¨ã—ã¦', 'ã«ã‚ˆã‚Š', 'ã«ã‚ˆã£ã¦', 'ã«å¯¾ã—ã¦',
            'ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ', 'ã©ã‚Œ', 'ãã®', 'ã“ã®', 'ã‚ã®', 'ã©ã®',
            'ç§', 'åƒ•', 'ä¿º', 'å½¼', 'å½¼å¥³', 'æˆ‘ã€…', 'çš†ã•ã‚“', 'ã¿ãªã•ã‚“',
            'ã¯ã„', 'ã„ã„ãˆ', 'ãã†ã§ã™ã­', 'ãªã‚‹ã»ã©', 'ã‚ã‹ã‚Šã¾ã—ãŸ',
            'ã‚ãƒ¼', 'ãˆãƒ¼', 'ã†ãƒ¼', 'ã‚“ãƒ¼', 'ãˆãˆ', 'ã‚ã®', 'ãã®', 'ã¾ã‚'
        }
        
        # å°‚é–€ç”¨èªã®ã‚«ãƒ†ã‚´ãƒª
        self.tech_keywords = {
            'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°': ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°', 'é–‹ç™º', 'å®Ÿè£…', 'ãƒã‚°', 'ãƒ‡ãƒãƒƒã‚°'],
            'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†': ['ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³', 'é€²æ—', 'ã‚¿ã‚¹ã‚¯', 'ã‚¢ã‚µã‚¤ãƒ³', 'ãƒªãƒªãƒ¼ã‚¹'],
            'ãƒ„ãƒ¼ãƒ«ãƒ»æŠ€è¡“': ['GitHub', 'Git', 'Docker', 'API', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ã‚µãƒ¼ãƒãƒ¼'],
            'ä¼šè­°ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³': ['ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°', 'ç›¸è«‡', 'æ±ºå®š', 'æ‰¿èª', 'å ±å‘Š'],
            'å“è³ªãƒ»ãƒ†ã‚¹ãƒˆ': ['ãƒ†ã‚¹ãƒˆ', 'ãƒ†ã‚¹ãƒˆé§†å‹•', 'QA', 'å“è³ª', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£']
        }
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.action_indicators = {
            'ã‚„ã‚‹', 'å®Ÿè£…ã™ã‚‹', 'ä½œæˆã™ã‚‹', 'ä¿®æ­£ã™ã‚‹', 'èª¿æŸ»ã™ã‚‹', 'æ¤œè¨ã™ã‚‹',
            'ç¢ºèªã™ã‚‹', 'å¯¾å¿œã™ã‚‹', 'ã‚„ã‚Šã¾ã™', 'æ‹…å½“ã™ã‚‹', 'é€²ã‚ã‚‹', 'å®Œäº†ã™ã‚‹',
            'ãŠé¡˜ã„ã—ã¾ã™', 'ã‚„ã£ã¦ãã ã•ã„', 'ã—ã¦ãã ã•ã„', 'å¯¾å¿œã—ã¦ãã ã•ã„'
        }

    async def extract_keywords_and_actions(
        self, 
        transcription: str,
        speaker_segments: Optional[List] = None
    ) -> Tuple[List[KeywordInfo], List[ActionItem]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º"""
        try:
            if not self.enable_keyword_extraction:
                return [], []
            
            logger.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºã‚’é–‹å§‹")
            
            # 1. åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = await self._extract_basic_keywords(transcription)
            
            # 2. AI ã«ã‚ˆã‚‹é«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            if self.llm_provider:
                ai_keywords = await self._extract_ai_keywords(transcription)
                keywords = self._merge_keywords(keywords, ai_keywords)
            
            # 3. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º
            action_items = []
            if self.enable_action_items:
                action_items = await self._extract_action_items(transcription, speaker_segments)
            
            logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå®Œäº†: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰, {len(action_items)} ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
            return keywords, action_items
            
        except Exception as e:
            logger.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return [], []

    async def _extract_basic_keywords(self, text: str) -> List[KeywordInfo]:
        """åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        # æ–‡ã‚’åˆ†å‰²
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        # å˜èªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        words = []
        for sentence in sentences:
            # ã‚«ã‚¿ã‚«ãƒŠèªã‚’æŠ½å‡º
            katakana_words = re.findall(r'[ã‚¢-ãƒ³]{2,}', sentence)
            words.extend(katakana_words)
            
            # è‹±èªå˜èªã‚’æŠ½å‡º
            english_words = re.findall(r'[A-Za-z]{2,}', sentence)
            words.extend(english_words)
            
            # æ¼¢å­—ã‚’å«ã‚€å˜èªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            kanji_words = re.findall(r'[ä¸€-é¾¯]{2,}', sentence)
            words.extend(kanji_words)
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»ã—ã€é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        filtered_words = [w for w in words if w.lower() not in self.stop_words and len(w) >= 2]
        word_counts = Counter(filtered_words)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’ç”Ÿæˆ
        keywords = []
        for word, freq in word_counts.most_common(20):  # ä¸Šä½20å€‹
            category = self._categorize_keyword(word)
            contexts = self._find_contexts(word, sentences)
            importance = self._calculate_importance(word, freq, len(sentences))
            
            keywords.append(KeywordInfo(
                word=word,
                frequency=freq,
                importance_score=importance,
                category=category,
                contexts=contexts[:3]  # æœ€å¤§3ã¤ã®æ–‡è„ˆ
            ))
        
        return keywords

    async def _extract_ai_keywords(self, text: str) -> List[KeywordInfo]:
        """AI ã«ã‚ˆã‚‹é«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        try:
            prompt = f"""ä»¥ä¸‹ã®ä¼šè­°æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

ä»¥ä¸‹ã®å½¢å¼ã§ã€æœ€å¤§15å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
- æŠ€è¡“ç”¨èªã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€é‡è¦ãªæ¦‚å¿µã‚’å„ªå…ˆ
- ä¸€èˆ¬çš„ã™ãã‚‹å˜èªã¯é™¤å¤–
- å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’1-10ã§è©•ä¾¡

å›ç­”å½¢å¼:
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1 (é‡è¦åº¦: X) - ã‚«ãƒ†ã‚´ãƒª: Y
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2 (é‡è¦åº¦: X) - ã‚«ãƒ†ã‚´ãƒª: Y
..."""

            if hasattr(self.llm_provider, 'generate_text'):
                response = await self.llm_provider.generate_text(prompt, max_tokens=1000, temperature=0.3)
                return self._parse_ai_keywords(response)
            
        except Exception as e:
            logger.error(f"AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return []

    def _parse_ai_keywords(self, ai_response: str) -> List[KeywordInfo]:
        """AI ã®å›ç­”ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’è§£æ"""
        keywords = []
        lines = ai_response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°: "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (é‡è¦åº¦: X) - ã‚«ãƒ†ã‚´ãƒª: Y"
            match = re.match(r'(.+?)\s*\(é‡è¦åº¦:\s*(\d+)\)\s*-\s*ã‚«ãƒ†ã‚´ãƒª:\s*(.+)', line)
            if match:
                word = match.group(1).strip()
                importance = float(match.group(2))
                category = match.group(3).strip()
                
                keywords.append(KeywordInfo(
                    word=word,
                    frequency=1,  # AIæŠ½å‡ºã§ã¯é »åº¦ã¯1
                    importance_score=importance,
                    category=category,
                    contexts=[]
                ))
        
        return keywords

    def _merge_keywords(self, basic_keywords: List[KeywordInfo], ai_keywords: List[KeywordInfo]) -> List[KeywordInfo]:
        """åŸºæœ¬æŠ½å‡ºã¨AIæŠ½å‡ºã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸"""
        keyword_dict = {}
        
        # åŸºæœ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        for kw in basic_keywords:
            keyword_dict[kw.word.lower()] = kw
        
        # AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯é‡è¦åº¦ã‚’æ›´æ–°ï¼‰
        for kw in ai_keywords:
            key = kw.word.lower()
            if key in keyword_dict:
                # æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’æ›´æ–°
                existing = keyword_dict[key]
                existing.importance_score = max(existing.importance_score, kw.importance_score)
                if kw.category and existing.category == 'ä¸€èˆ¬':
                    existing.category = kw.category
            else:
                keyword_dict[key] = kw
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        return sorted(keyword_dict.values(), key=lambda x: x.importance_score, reverse=True)

    async def _extract_action_items(
        self, 
        text: str, 
        speaker_segments: Optional[List] = None
    ) -> List[ActionItem]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º"""
        action_items = []
        
        if not self.llm_provider:  # AI ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç°¡æ˜“æŠ½å‡º
            return self._extract_basic_actions(text)
        
        try:
            prompt = f"""ä»¥ä¸‹ã®ä¼šè­°æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆã‚¿ã‚¹ã‚¯ãƒ»TODOï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

ä»¥ä¸‹ã®è¦³ç‚¹ã§æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
- å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯ã‚„ä½œæ¥­é …ç›®
- æ‹…å½“è€…ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
- æœŸé™ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹ã‚‚ã®
- æ±ºå®šäº‹é …ã‚„æ¬¡å›ã¾ã§ã«è¡Œã†ã“ã¨

å›ç­”å½¢å¼:
ã‚¿ã‚¹ã‚¯: [å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯å†…å®¹]
æ‹…å½“è€…: [æ‹…å½“è€…å ã¾ãŸã¯ ä¸æ˜]
æœŸé™: [æœŸé™ ã¾ãŸã¯ ä¸æ˜]
å„ªå…ˆåº¦: [é«˜/ä¸­/ä½]
æ–‡è„ˆ: [è©²å½“ã™ã‚‹æ–‡ã®æŠœç²‹]
---"""

            response = await self.llm_provider.generate_text(prompt, max_tokens=1500, temperature=0.2)
            action_items = self._parse_action_items(response)
            
        except Exception as e:
            logger.error(f"ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            action_items = self._extract_basic_actions(text)
        
        return action_items

    def _extract_basic_actions(self, text: str) -> List[ActionItem]:
        """åŸºæœ¬çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º"""
        actions = []
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        for sentence in sentences:
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æŒ‡ç¤ºèªã‚’å«ã‚€æ–‡ã‚’æ¤œç´¢
            for indicator in self.action_indicators:
                if indicator in sentence:
                    # ç°¡æ˜“çš„ã«ã‚¿ã‚¹ã‚¯ã¨ã—ã¦æŠ½å‡º
                    task = sentence
                    assignee = self._extract_assignee(sentence)
                    
                    actions.append(ActionItem(
                        task=task,
                        assignee=assignee,
                        deadline=None,
                        priority='ä¸­',
                        context=sentence
                    ))
                    break
        
        return actions[:10]  # æœ€å¤§10å€‹

    def _extract_assignee(self, sentence: str) -> Optional[str]:
        """æ–‡ã‹ã‚‰æ‹…å½“è€…ã‚’æŠ½å‡º"""
        # ç°¡æ˜“çš„ãªæ‹…å½“è€…æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'([ã-ã‚“]+)ã•ã‚“.{0,20}(ã‚„ã‚‹|ã‚„ã‚Šã¾ã™|æ‹…å½“)',
            r'([ã-ã‚“]+)ãŒ.{0,20}(ã‚„ã‚‹|ã‚„ã‚Šã¾ã™|æ‹…å½“)',
            r'([A-Za-z]+)ã•ã‚“.{0,20}(ã‚„ã‚‹|ã‚„ã‚Šã¾ã™|æ‹…å½“)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, sentence)
            if match:
                return match.group(1) + 'ã•ã‚“'
        
        return None

    def _parse_action_items(self, ai_response: str) -> List[ActionItem]:
        """AIå›ç­”ã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’è§£æ"""
        actions = []
        items = ai_response.split('---')
        
        for item in items:
            lines = [line.strip() for line in item.strip().split('\n') if line.strip()]
            if len(lines) < 4:
                continue
            
            task = assignee = deadline = priority = context = None
            
            for line in lines:
                if line.startswith('ã‚¿ã‚¹ã‚¯:'):
                    task = line.replace('ã‚¿ã‚¹ã‚¯:', '').strip()
                elif line.startswith('æ‹…å½“è€…:'):
                    assignee = line.replace('æ‹…å½“è€…:', '').strip()
                    if assignee == 'ä¸æ˜':
                        assignee = None
                elif line.startswith('æœŸé™:'):
                    deadline = line.replace('æœŸé™:', '').strip()
                    if deadline == 'ä¸æ˜':
                        deadline = None
                elif line.startswith('å„ªå…ˆåº¦:'):
                    priority = line.replace('å„ªå…ˆåº¦:', '').strip()
                elif line.startswith('æ–‡è„ˆ:'):
                    context = line.replace('æ–‡è„ˆ:', '').strip()
            
            if task:
                actions.append(ActionItem(
                    task=task,
                    assignee=assignee,
                    deadline=deadline,
                    priority=priority or 'ä¸­',
                    context=context or task
                ))
        
        return actions

    def _categorize_keyword(self, word: str) -> str:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º"""
        for category, keywords in self.tech_keywords.items():
            if any(kw in word for kw in keywords):
                return category
        
        # ã‚«ã‚¿ã‚«ãƒŠèªã¯æŠ€è¡“ç”¨èªã¨ã—ã¦åˆ†é¡
        if re.match(r'^[ã‚¢-ãƒ³]+$', word):
            return 'ãƒ„ãƒ¼ãƒ«ãƒ»æŠ€è¡“'
        
        # è‹±èªã¯æŠ€è¡“ç”¨èªã¨ã—ã¦åˆ†é¡
        if re.match(r'^[A-Za-z]+$', word):
            return 'ãƒ„ãƒ¼ãƒ«ãƒ»æŠ€è¡“'
        
        return 'ä¸€èˆ¬'

    def _find_contexts(self, word: str, sentences: List[str]) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå‡ºç¾ã™ã‚‹æ–‡è„ˆã‚’æ¤œç´¢"""
        contexts = []
        for sentence in sentences:
            if word in sentence:
                contexts.append(sentence)
                if len(contexts) >= 3:
                    break
        return contexts

    def _calculate_importance(self, word: str, frequency: int, total_sentences: int) -> float:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã‚’è¨ˆç®—"""
        # åŸºæœ¬çš„ãªé‡è¦åº¦è¨ˆç®—
        base_score = frequency / total_sentences * 10
        
        # å°‚é–€ç”¨èªãƒœãƒ¼ãƒŠã‚¹
        if self._categorize_keyword(word) != 'ä¸€èˆ¬':
            base_score *= 1.5
        
        # é•·ã„å˜èªãƒœãƒ¼ãƒŠã‚¹
        if len(word) >= 4:
            base_score *= 1.2
        
        return min(base_score, 10.0)  # æœ€å¤§10ç‚¹

    def format_keywords(self, keywords: List[KeywordInfo]) -> str:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’æ•´å½¢"""
        if not keywords:
            return "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        lines = ["=== é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ===\n"]
        
        for i, kw in enumerate(keywords[:15], 1):  # ä¸Šä½15å€‹
            lines.append(f"{i}. **{kw.word}** (é‡è¦åº¦: {kw.importance_score:.1f})")
            lines.append(f"   ã‚«ãƒ†ã‚´ãƒª: {kw.category} | å‡ºç¾å›æ•°: {kw.frequency}å›")
            if kw.contexts:
                lines.append(f"   ä¾‹: {kw.contexts[0][:50]}...")
            lines.append("")
        
        return "\n".join(lines)

    def format_action_items(self, actions: List[ActionItem]) -> str:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ•´å½¢"""
        if not actions:
            return "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        lines = ["=== ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ  ===\n"]
        
        for i, action in enumerate(actions, 1):
            priority_emoji = {"é«˜": "ğŸ”´", "ä¸­": "ğŸŸ¡", "ä½": "ğŸŸ¢"}.get(action.priority, "âšª")
            
            lines.append(f"{i}. {priority_emoji} **{action.task}**")
            if action.assignee:
                lines.append(f"   ğŸ‘¤ æ‹…å½“è€…: {action.assignee}")
            if action.deadline:
                lines.append(f"   ğŸ“… æœŸé™: {action.deadline}")
            lines.append(f"   ğŸ“ æ–‡è„ˆ: {action.context[:100]}...")
            lines.append("")
        
        return "\n".join(lines)